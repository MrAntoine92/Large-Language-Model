{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "# Chapter 6 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Increasing the context length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b5925",
   "metadata": {},
   "source": [
    "**Padding Input Sequences in Neural Language Models**\n",
    "\n",
    "**Key Research Question: How does padding inputs to the maximum `token` length affect model predictive performance?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Implement systematic `token` padding \n",
    "- Analyze padding's impact on model performance\n",
    "- Explore input representation interactions\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Input `padding` strategy\n",
    "- Maximum `token` length\n",
    "- Predictive performance metrics\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Implement maximum-length input `padding`\n",
    "2. Measure performance variations\n",
    "3. Compare padded versus non-padded inputs\n",
    "4. Assess computational implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780455-f52a-48d1-ab82-6afd40bcad8b",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Finetuning the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23d900",
   "metadata": {},
   "source": [
    "**Model-Wide Fine-Tuning Performance Assessment**\n",
    "\n",
    "**Key Research Question: What is the impact of `fine-tuning` the entire transformer model versus a single final block on predictive performance?**\n",
    "\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Implement comprehensive model `fine-tuning`\n",
    "- Compare performance against single block tuning\n",
    "- Assess computational and representational changes\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Full model `fine-tuning` strategy\n",
    "- Performance evaluation metrics\n",
    "- Comparative analysis methodology\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. `Fine-tune` entire transformer model\n",
    "2. Measure predictive performance metrics\n",
    "3. Compare with previous single-block tuning results\n",
    "4. Analyze performance variation mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269bce3-f2b5-4a76-a692-5977c75a57b6",
   "metadata": {},
   "source": [
    "## Exercise 6.3: Finetuning the first versus last token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ddba60",
   "metadata": {},
   "source": [
    "**First Token Fine-Tuning: Predictive Performance Analysis**\n",
    "\n",
    "**Key Research Question: How do predictive performance characteristics change when fine-tuning the first output `token` compared to the last output `token`?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Fine-tune first output `token`\n",
    "- Compare performance against last `token` fine-tuning\n",
    "- Assess representational learning variations\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Initial `token` fine-tuning strategy\n",
    "- Performance evaluation metrics\n",
    "- Comparative analysis methodology\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Implement first `token` fine-tuning\n",
    "2. Measure predictive performance\n",
    "3. Compare with last `token` fine-tuning results\n",
    "4. Analyze performance variation mechanisms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
