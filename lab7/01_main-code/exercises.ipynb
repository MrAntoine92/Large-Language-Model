{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": [
    "# Chapter 7 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Changing prompt styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130e20e",
   "metadata": {},
   "source": [
    "**Prompt Style Comparative Analysis: Impact on Model Response Quality**\n",
    "\n",
    "**Key Research Question: How do different prompt styles `(Alpaca vs. Phi-3)` influence the generative response quality of the `fine-tuned` model?**\n",
    "\n",
    "*Methodological Approach:*\n",
    "- `Fine-tune` model with `Alpaca` prompt style\n",
    "- Apply `Phi-3 prompt` configuration\n",
    "- Compare response quality metrics\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Prompt style variations\n",
    "- Response quality assessment\n",
    "- Comparative performance evaluation\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Implement Phi-3 prompt style `shown in figure 4 in chapter 7`\n",
    "2. Evaluate response quality\n",
    "3. Compare with `Alpaca` prompt results\n",
    "4. Analyze observed variations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 7.2: Instruction and input masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0650820",
   "metadata": {},
   "source": [
    "**Instruction Masking Performance Evaluation**\n",
    "\n",
    "**Key Research Question**: How does replacing instruction and input `tokens` with the `-100` mask impact model performance during fine-tuning?\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Implement `-100` token masking for instructions\n",
    "- Evaluate model performance\n",
    "- Compare against standard fine-tuning approach\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Instruction masking technique\n",
    "- Performance assessment metrics\n",
    "- Comparative analysis methodology\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Apply `-100` mask to instruction and input `tokens`\n",
    "2. Fine-tune model using `InstructionDataset`\n",
    "3. Measure and compare performance metrics\n",
    "4. Analyze potential learning improvements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 7.3: Finetuning on the original Alpaca dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c561a87",
   "metadata": {},
   "source": [
    "**Large-Scale Instruction Dataset Fine-Tuning: Computational and Methodological Considerations**\n",
    "\n",
    "The Alpaca dataset, a significant instruction dataset created by Stanford researchers. With 52,002 entries, this dataset is notably larger than the previously mentioned instruction-data.json file. The text provides recommendations for fine-tuning a Large Language Model (LLM) using this dataset.\n",
    "\n",
    "**Key Research Question: How can one effectively fine-tune an LLM using the Alpaca dataset while managing computational resources and potential memory constraints?**\n",
    "\n",
    "**Link to download the Alpaca dataset**:  [here](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json).\n",
    "\n",
    "*Methodological Approach:*\n",
    "- Analyze Alpaca dataset characteristics\n",
    "- Develop GPU-accelerated fine-tuning strategy\n",
    "- Implement computational optimization techniques\n",
    "\n",
    "*Critical Parameters:*\n",
    "- Dataset scale (52,002 entries)\n",
    "- Computational resource management\n",
    "- Fine-tuning performance optimization\n",
    "\n",
    "*Computational Optimization Strategies:*\n",
    "- Batch size reduction (`batch_size`)\n",
    "- Maximum sequence length adjustment\n",
    "- GPU resource utilization\n",
    "\n",
    "*Recommended Investigation:*\n",
    "1. Load and prepare Alpaca dataset\n",
    "2. Implement adaptive fine-tuning approach\n",
    "3. Address potential memory constraints\n",
    "4. Optimize computational performance\n",
    "\n",
    "*Key Mitigation Techniques:\n",
    "- Reduce `batch_size` (8 → 4 → 2 → 1)\n",
    "- Truncate `allowed_max_length` (1,024 → 512 → 256)\n",
    "- Leverage GPU computational capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
